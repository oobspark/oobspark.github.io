{"meta":{"title":"Oobspark","subtitle":"Out Of the Box and Spark","description":"Out Of the Box and Spark","author":"oobspark","url":"https://oobspark.github.io"},"pages":[{"title":"404","date":"2019-11-12T07:32:27.578Z","updated":"2019-11-12T07:32:27.578Z","comments":true,"path":"404.html","permalink":"https://oobspark.github.io/404.html","excerpt":"","text":"404页面 *{margin:0;padding:0;outline:none;font-family:\\5FAE\\8F6F\\96C5\\9ED1,宋体;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;-khtml-user-select:none;user-select:none;cursor:default;font-weight:lighter;} .center{margin:0 auto;} .whole{width:100%;height:100%;line-height:100%;position:fixed;bottom:0;left:0;z-index:-1000;overflow:hidden;} .whole img{width:100%;height:100%;} .mask{width:100%;height:100%;position:absolute;top:0;left:0;background:#000;opacity:0.6;filter:alpha(opacity=60);} .b{width:100%;text-align:center;height:400px;position:absolute;top:50%;margin-top:-230px}.a{width:150px;height:50px;margin-top:30px}.a a{display:block;float:left;width:150px;height:50px;background:#fff;text-align:center;line-height:50px;font-size:18px;border-radius:25px;color:#333}.a a:hover{color:#000;box-shadow:#fff 0 0 20px} p{color:#fff;margin-top:40px;font-size:24px;} #num{margin:0 5px;font-weight:bold;} var num=4; function redirect(){ num--; document.getElementById(\"num\").innerHTML=num; if(num 暂时未能找到您查找的页面 可能输入的网址错误或此页面不存在 秒后自动跳转到主页"},{"title":"Tags","date":"2018-04-04T07:12:32.000Z","updated":"2019-11-12T07:32:27.597Z","comments":true,"path":"tags/index.html","permalink":"https://oobspark.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Swoole精华手记","slug":"swoole-basic","date":"2020-07-14T02:22:42.000Z","updated":"2020-07-14T03:29:08.276Z","comments":true,"path":"2020/07/14/swoole-basic/","link":"","permalink":"https://oobspark.github.io/2020/07/14/swoole-basic/","excerpt":"","text":"知识点： 可选回调 123456789101112131415port 未调用 on 方法，设置回调函数的监听端口，默认使用主服务器的回调函数，port 可以通过 on 方法设置的回调有：TCP 服务器 onConnect onClose onReceiveUDP 服务器 onPacket onReceiveHTTP 服务器 onRequestWebSocket 服务器 onMessage onOpen onHandshake 事件执行顺序 12345678所有事件回调均在 $server-&gt;start 后发生服务器关闭程序终止时最后一次事件是 onShutdown服务器启动成功后，onStart/onManagerStart/onWorkerStart 会在不同的进程内并发执行onReceive/onConnect/onClose 在 Worker 进程中触发Worker/Task 进程启动 / 结束时会分别调用一次 onWorkerStart/onWorkerStoponTask 事件仅在 task 进程中发生onFinish 事件仅在 worker 进程中发生onStart/onManagerStart/onWorkerStart 3 个事件的执行顺序是不确定的","categories":[],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"https://oobspark.github.io/tags/Swoole/"}]},{"title":"Spark Core","slug":"spark-core","date":"2020-04-20T14:51:02.000Z","updated":"2020-07-15T07:17:31.751Z","comments":true,"path":"2020/04/20/spark-core/","link":"","permalink":"https://oobspark.github.io/2020/04/20/spark-core/","excerpt":"","text":"基本操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark --master local[4]from pyspark import SparkContext, SparkConfconf = SparkConf().setAppName(&apos;myspark&apos;).setMaster(&quot;local[4]&quot;)sc = SparkContext(conf=conf)PySpark 支持 Hadoop, local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.data = [1, 2, 3, 4, 5]# 多cpu并行计算，如sc.parallelize(data, 4)distData = sc.parallelize(data)distData.reduce(lambda a, b: a + b)distFile = sc.textFile(&quot;README.md&quot;)# 计算行数distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, &quot;a&quot; * x))rdd.saveAsSequenceFile(&quot;1.txt&quot;)sorted(sc.sequenceFile(&quot;1.txt&quot;).collect())./bin/pyspark --jars /path/to/elasticsearch-hadoop.jarconf = &#123;&quot;es.resource&quot; : &quot;index/type&quot;&#125; # assume Elasticsearch is running on localhost defaultsrdd = sc.newAPIHadoopRDD(&quot;org.elasticsearch.hadoop.mr.EsInputFormat&quot;, &quot;org.apache.hadoop.io.NullWritable&quot;, &quot;org.elasticsearch.hadoop.mr.LinkedMapWritable&quot;, conf=conf)rdd.first() # the result is a MapWritable that is converted to a Python dict(u&apos;Elasticsearch ID&apos;, &#123;u&apos;field1&apos;: True, u&apos;field2&apos;: u&apos;Some Text&apos;, u&apos;field3&apos;: 12345&#125;)lines = sc.textFile(&quot;data.txt&quot;)lineLengths = lines.map(lambda s: len(s))# 等下还需要使用时，可以持久化lineLengths.persist()totalLength = lineLengths.reduce(lambda a, b: a + b)# 不能使用全局变量 global，应该使用accumulatoraccum = sc.accumulator(0)sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))accum.value #100rdd.collect().foreach(println) #这样打印有可能内存溢出#打印少数元素rdd.take(100).foreach(println)pairs = sc.parallelize([1, 2, 3, 4]).map(lambda s: (s, 1))counts = pairs.reduceByKey(lambda a, b: a + b)","categories":[],"tags":[{"name":"Spark Core","slug":"Spark-Core","permalink":"https://oobspark.github.io/tags/Spark-Core/"}]},{"title":"HBASE 数据设计","slug":"hbase-design","date":"2019-12-02T03:25:46.000Z","updated":"2020-07-15T07:17:03.310Z","comments":true,"path":"2019/12/02/hbase-design/","link":"","permalink":"https://oobspark.github.io/2019/12/02/hbase-design/","excerpt":"","text":"hbase 数据设计读取访问模式： 用户关注谁？ 特定用户A是否关注用户B？ 谁关注了特定用户A？ 写访问模式： 用户关注新用户。 用户取消关注某人。","categories":[],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"https://oobspark.github.io/tags/HBASE/"},{"name":"教程简介","slug":"教程简介","permalink":"https://oobspark.github.io/tags/教程简介/"}]},{"title":"Elasticsearch基本操作","slug":"elasticsearch","date":"2019-03-11T02:12:41.000Z","updated":"2020-07-15T07:22:06.566Z","comments":true,"path":"2019/03/11/elasticsearch/","link":"","permalink":"https://oobspark.github.io/2019/03/11/elasticsearch/","excerpt":"","text":"基本操作elasticsearch v6.8.7123456创建索引curl -X PUT &quot;localhost:9200/customer/_doc/1?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;name&quot;: &quot;John Doe&quot;&#125;&apos; 获取索引数据1curl -X GET &quot;localhost:9200/customer/_doc/1?pretty&quot; 批量创建索引 5MB~15MB, 1,000~5,000条记录为宜下载accounts.json 文件，12345&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;account_number&quot;:1,&quot;balance&quot;:39225,&quot;firstname&quot;:&quot;Amber&quot;,&quot;lastname&quot;:&quot;Duke&quot;,&quot;age&quot;:32,&quot;gender&quot;:&quot;M&quot;,&quot;address&quot;:&quot;880 Holmes Lane&quot;,&quot;employer&quot;:&quot;Pyrami&quot;,&quot;email&quot;:&quot;amberduke@pyrami.com&quot;,&quot;city&quot;:&quot;Brogan&quot;,&quot;state&quot;:&quot;IL&quot;&#125;curl -H &quot;Content-Type: application/json&quot; -XPOST &quot;localhost:9200/bank/_doc/_bulk?pretty&amp;refresh&quot; --data-binary &quot;@accounts.json&quot; 查看索引索引情况1curl &quot;localhost:9200/_cat/indices?v&quot; 搜索123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051curl -X POST &quot;localhost:9200/bank/_search?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;account_number&quot;: &quot;asc&quot; &#125; ], &quot;from&quot;: 10, &quot;size&quot;: 10&#125;&apos;curl -X GET &quot;localhost:9200/bank/_search?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill lane&quot; &#125; &#125;&#125;&apos;curl -X GET &quot;localhost:9200/bank/_search?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;address&quot;: &quot;mill lane&quot; &#125; &#125;&#125;&apos;curl -X GET &quot;localhost:9200/bank/_search?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;age&quot;: &quot;40&quot; &#125; &#125; ], &quot;must_not&quot;: [ &#123; &quot;match&quot;: &#123; &quot;state&quot;: &quot;ID&quot; &#125; &#125; ] &#125; &#125;&#125;&apos;curl -X GET &quot;localhost:9200/bank/_search?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;balance&quot;: &#123; &quot;gte&quot;: 20000, &quot;lte&quot;: 30000 &#125; &#125; &#125; &#125; &#125;&#125;&apos; 查看索引mapping情况（索引中各字段的映射定义）1curl -X GET &quot;localhost:9200/bank/_mapping?pretty&quot; 聚合查询 Refer 记得使用state.keyword，使用完整keyword，其中size=0 表示不需要返回参与查询的文档12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152curl -X GET &quot;localhost:9200/bank/_search?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_state&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state.keyword&quot; &#125; &#125; &#125;&#125;&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;return_expires_in&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;expires_in&quot; &#125; &#125; &#125;&#125;&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;return_min_expires_in&quot;: &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;expires_in&quot; &#125; &#125; &#125;&#125;&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;return_max_expires_in&quot;: &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;expires_in&quot; &#125; &#125; &#125;&#125;&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;return_avg_expires_in&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;expires_in&quot; &#125; &#125; &#125;&#125;&apos; 索引自动创建添加索引数据时，索引mapping会自己创建1234567891011121314151617181920PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;action.auto_create_index&quot;: &quot;twitter,index10,-index1*,+ind*&quot; &#125;&#125;PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;action.auto_create_index&quot;: &quot;false&quot; &#125;&#125;PUT _cluster/settings&#123; &quot;persistent&quot;: &#123; &quot;action.auto_create_index&quot;: &quot;true&quot; &#125;&#125; 实操es之零停机重新索引数据生产环境的索引一定要记得创建alias，不然后面就等着哭吧！以下所有操作都是基于一个前提：在建原始索引的时候，给原始索引创建了别名 12345678910111213141516171819202122232425262728293031PUT /my_index_v1 //创建索引 my_index_v1PUT /my_index_v1/_alias/my_index //设置 my_index为 my_index_v1创建mapping1. 原始的索引bank,类型：account,mapping如下&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 5 &#125;, &quot;mappings&quot;: &#123; &quot;account&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;content2&quot;: &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 新建一个空的索引bak_bak，类型：account,分片20,age字段由long改成了string类型，具有最新的、正确的配置 1234567891011121314151617181920212223242526&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 6 &#125;, &quot;mappings&quot;: &#123; &quot;account&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;content2&quot;: &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 设置别名 12345678910POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;articles1&quot;, &quot;alias&quot;: &quot;my_index&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;articles2&quot;, &quot;alias&quot;: &quot;my_index&quot; &#125;&#125; ]&#125;PUT /articles2 //创建索引 articles2PUT /articles2/_alias/my_index //设置 my_index为 articles2 查询当前别名下的所有索引： 1GET /*/_alias/my_index 数据重新索引 123456789POST _reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;articles1&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;articles2&quot; &#125;&#125; 查看数据是否进入新的索引1GET articles2/article/1 接下来修改alias别名的指向（如果你之前没有用alias来改mapping,纳尼就等着哭吧）12345678910111213curl -XPOST localhost:8305/_aliases -d &apos;&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;alias&quot;: &quot;my_index&quot;, &quot;index&quot;: &quot;articles1&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;alias&quot;: &quot;my_index&quot;, &quot;index&quot;: &quot;articles2&quot; &#125;&#125; ]&#125;","categories":[],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://oobspark.github.io/tags/elasticsearch/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://oobspark.github.io/tags/搜索引擎/"}]},{"title":"LNMP技术栈在Docker中的使用","slug":"lnmp-stack-with-docker","date":"2019-01-20T02:37:16.000Z","updated":"2019-11-12T07:32:27.584Z","comments":true,"path":"2019/01/20/lnmp-stack-with-docker/","link":"","permalink":"https://oobspark.github.io/2019/01/20/lnmp-stack-with-docker/","excerpt":"","text":"目标LNMP技术栈是Web开发中流行的技术栈之一，本文的目标是，利用docker搭建一套LNMP服务。 好，废话不多说，我们直入主题。 Docker的安装Docker CE（Community Edition）社区版本本身支持多种平台的安装，如Linux，MacOS，Windows等操作系统，此外，还支持AWS，Azure等云计算平台。 如果你使用的是Windows 10，那么你可以直接Docker Desktop for Windows。要使用此工具，你需要开启你Windows中的Hyper-V服务和BIOS中的Virtualization选项。 笔者使用的是Windows 7操作系统，直接使用Docker Toolbox，下载并安装即可。 使用到的镜像本文中会使用到以下三个基础镜像： nginx:1.15 php:7.1-fpm mysql:5.7 三个镜像都是官方提供的镜像，官方镜像保证了稳定性的同时，同时也保留了一些扩展性，使用起来比较方便。 我们先把三个镜像下载到本地备用。打开Docker Quickstart Terminal，并执行： 123docker pull nginx:1.15docker pull php:7.1-fpmdocker pull mariadb:10.3 常规方法首先我们使用docker的基本命令来创建我们的容器。 MariaDB 打开Docker Quickstart Terminal后，执行： 123cd lnmpdocker run --name mysql -p 3306:3306 \\ -v $PWD/mysql:/var/lib/mysql -d mariadb:10.3 查看服务状态： 1mysql -h192.168.99.100 -uroot -p123123 -e &quot;status&quot; 此处返回服务器状态信息 PHP-FPM 12docker run --name php-fpm --link mysql:mysql -p 9000:9000 \\ -v $PWD/html:/var/www/html:ro -d php:7.1-fpm --name php-fpm： 自定义容器名 --link mysql:mysql 与mysql容器关联，并将mysql容器的域名指定为mysql -v $PWD/www:/var/www/html:ro `$PWD/www`是宿主机的php文件目录 `/var/www/html`是容器内php文件目录 `ro`表示只读。 官方docker中已经包含的PHP的部分基本扩展，但是很显然这并不能满足大多数的使用场景。 因此，官方还提供了docker-php-ext-configure，docker-php-ext-install和docker-php-ext-enable等脚本供我们使用，可以更方便的安装我们的扩展。 此外，容器还提供对pecl命令的支持。 我们基于此安装我们常用一些扩展。 1234docker-php-ext-install pdo pdo_mysqlpecl install redis-4.0.1 &amp;&amp; \\ pecl install xdebug-2.6.0 \\ docker-php-ext-enable redis xdebug 当然我们也可以选择直接编译安装。 123456789101112curl -fsSL &apos;http://pecl.php.net/get/redis-4.2.0.tgz&apos; \\ &amp;&amp; tar zxvf redis-4.2.0.tgz \\ &amp;&amp; rm redis-4.2.0.tgz \\ &amp;&amp; ( \\ cd redis-4.2.0 \\ &amp;&amp; phpize \\ &amp;&amp; ./configure \\ &amp;&amp; make -j &quot;$(nproc)&quot; \\ &amp;&amp; make install \\ ) \\ &amp;&amp; rm -r redis-4.2.0 \\ &amp;&amp; docker-php-ext-enable redis Nginx 1234docker run --name nginx -p 80:80 --link php-fpm:php \\ -v $PWD/default_host.conf:/etc/nginx/conf.d/default.conf:ro \\ -v $PWD/html:/usr/share/nginx/html:ro \\ -d nginx:1.15 --name nginx： 自定义容器名 --link php-fpm:php 与php-fpm容器关联，并将php-fpm容器的域名指定为php -v $PWD/default_host.conf:/etc/nginx/conf.d/default.conf:ro 替换host文件 -v $PWD/html:/usr/share/nginx/html:ro \\ 替换网站根目录 总结 至此，我们依次启动了mysql，php-fpm和nginx容器（顺序很重要，因为他们有依赖关系）。打开浏览器，访问http://192.168.99.100/，就是见证奇迹的时刻。 高阶以上是比较常规的一种方式，也稍显麻烦。下面介绍docker-composer的配置方式。 12345678910111213141516171819202122232425262728293031323334353637383940414243version: &apos;3&apos;services: mysql: image: mariadb:10.3 volumes: - mysql-data:/var/lib/mysql environment: TZ: &apos;Asia/Shanghai&apos; MYSQL_ROOT_PASSWORD: 123123 command: [&apos;mysqld&apos;, &apos;--character-set-server=utf8&apos;] ports: - &quot;3306:3306&quot; networks: - backend php: image: &quot;mylnmp/php:v1.0&quot; build: context: . dockerfile: Dockerfile-php ports: - &quot;9000:9000&quot; networks: - frontend - backend depends_on: - mysql nginx: image: &quot;mylnmp/nginx:v1.0&quot; build: context: . dockerfile: Dockerfile-nginx ports: - &quot;80:80&quot; networks: - frontend depends_on: - phpvolumes: mysql-data:networks: frontend: backend: 具体可参考我的GitHub项目lnmp-container","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://oobspark.github.io/tags/docker/"},{"name":"lnmp","slug":"lnmp","permalink":"https://oobspark.github.io/tags/lnmp/"}]},{"title":"numpy基础","slug":"numpy-bootstrap","date":"2018-08-01T01:41:12.000Z","updated":"2019-11-12T07:32:27.585Z","comments":true,"path":"2018/08/01/numpy-bootstrap/","link":"","permalink":"https://oobspark.github.io/2018/08/01/numpy-bootstrap/","excerpt":"","text":"Numpy 简介NumPy是一个Python包。它代表“Numeric Python”。它是一个由多维数组对象和用于处理数组的例程集合组成的库。 Numeric，即 NumPy 的前身，是由 Jim Hugunin 开发的。也开发了另一个包Numarray，它拥有一些额外的功能。2005年，Travis Oliphant通过将 Numarray的功能集成到Numeric包中来创建NumPy包。目前这个开源项目已经有非常多的贡献者。 环境搭建在安装了python和pip之后，一个命令搞定。 pip install numpy 然后我们进入Python交互式shell。 123import numpy as np a = np.array([1,2,3]) print a 如果你能正确执行上述代码，那么你的numpy环境就已经搭建好了。 基本属性ndarray.ndim：数组维度ndarray.shape：数组行和列的长度ndarray.size：同shapendarray.dtype：数组中元素的类型ndarray.itemsize：数组中单个元素所占字节数 1234567891011121314151617181920212223&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.shape(3, 5)&gt;&gt;&gt; a.ndim2&gt;&gt;&gt; a.dtype.name'int64'&gt;&gt;&gt; a.itemsize8&gt;&gt;&gt; a.size15&gt;&gt;&gt; type(a)&lt;type 'numpy.ndarray'&gt;&gt;&gt;&gt; b = np.array([6, 7, 8])&gt;&gt;&gt; barray([6, 7, 8])&gt;&gt;&gt; type(b)&lt;type 'numpy.ndarray'&gt; 创建数组创建数组的方式有很多，我们直接看代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.array([2,3,4])&gt;&gt;&gt; aarray([2, 3, 4])&gt;&gt;&gt; a.dtypedtype('int64')&gt;&gt;&gt; b = np.array([1.2, 3.5, 5.1])&gt;&gt;&gt; b.dtypedtype('float64')&gt;&gt;&gt; a = np.array(1,2,3,4) # WRONG&gt;&gt;&gt; a = np.array([1,2,3,4]) # RIGHT&gt;&gt;&gt; b = np.array([(1.5,2,3), (4,5,6)])&gt;&gt;&gt; barray([[ 1.5, 2. , 3. ], [ 4. , 5. , 6. ]])&gt;&gt;&gt; c = np.array( [ [1,2], [3,4] ], dtype=complex ) # 复数&gt;&gt;&gt; carray([[ 1.+0.j, 2.+0.j], [ 3.+0.j, 4.+0.j]])&gt;&gt;&gt; np.zeros( (3,4) )array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]])&gt;&gt;&gt; np.ones( (2,3,4), dtype=np.int16 ) # dtype 也可以被指定array([[[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]], [[ 1, 1, 1, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 1]]], dtype=int16)&gt;&gt;&gt; np.empty( (2,3) ) # 未初始化，输出可能会稍许怪异array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]])&gt;&gt;&gt; np.arange( 10, 30, 5 )array([10, 15, 20, 25])&gt;&gt;&gt; np.arange( 0, 2, 0.3 ) # 可接受float型步长参数array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8])&gt;&gt;&gt; np.linspace( 0, 2, 9 ) # 从0到2的9个数字array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ])&gt;&gt;&gt; np.random.rand(3,2)array([[ 0.14022471, 0.96360618], #random [ 0.37601032, 0.25528411], #random [ 0.49313049, 0.94909878]]) #random 基本操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&gt;&gt;&gt; a = np.array( [20,30,40,50] )&gt;&gt;&gt; b = np.arange( 4 )&gt;&gt;&gt; barray([0, 1, 2, 3])&gt;&gt;&gt; c = a-b&gt;&gt;&gt; carray([20, 29, 38, 47])&gt;&gt;&gt; b**2array([0, 1, 4, 9])&gt;&gt;&gt; 10*np.sin(a)array([ 9.12945251, -9.88031624, 7.4511316 , -2.62374854])&gt;&gt;&gt; a&lt;35array([ True, True, False, False])&gt;&gt;&gt; A = np.array( [[1,1], [0,1]] )&gt;&gt;&gt; B = np.array( [[2,0], [3,4]] )&gt;&gt;&gt; A * Barray([[2, 0], [0, 4]])&gt;&gt;&gt; A @ Barray([[5, 4], [3, 4]])&gt;&gt;&gt; A.dot(B)array([[5, 4], [3, 4]])&gt;&gt;&gt; a = np.ones((2,3), dtype=int)&gt;&gt;&gt; b = np.random.random((2,3))&gt;&gt;&gt; a *= 3&gt;&gt;&gt; aarray([[3, 3, 3], [3, 3, 3]])&gt;&gt;&gt; b += a&gt;&gt;&gt; barray([[ 3.417022 , 3.72032449, 3.00011437], [ 3.30233257, 3.14675589, 3.09233859]])&gt;&gt;&gt; a += b # b不会自动从float转变为intTraceback (most recent call last): ...TypeError: Cannot cast ufunc add output from dtype('float64') to dtype('int64') with casting rule 'same_kind'&gt;&gt;&gt; from numpy import pi&gt;&gt;&gt; a = np.ones(3, dtype=np.int32)&gt;&gt;&gt; b = np.linspace(0,pi,3)&gt;&gt;&gt; b.dtype.name'float64'&gt;&gt;&gt; c = a+b&gt;&gt;&gt; carray([ 1. , 2.57079633, 4.14159265])&gt;&gt;&gt; c.dtype.name'float64'&gt;&gt;&gt; d = np.exp(c*1j)&gt;&gt;&gt; darray([ 0.54030231+0.84147098j, -0.84147098+0.54030231j, -0.54030231-0.84147098j])&gt;&gt;&gt; d.dtype.name'complex128'&gt;&gt;&gt; a = np.random.random((2,3))&gt;&gt;&gt; aarray([[ 0.18626021, 0.34556073, 0.39676747], [ 0.53881673, 0.41919451, 0.6852195 ]])&gt;&gt;&gt; a.sum()2.5718191614547998&gt;&gt;&gt; a.min()0.1862602113776709&gt;&gt;&gt; a.max()0.6852195003967595&gt;&gt;&gt; b = np.arange(12).reshape(3,4)&gt;&gt;&gt; barray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt;&gt;&gt;&gt; b.sum(axis=0) # 每列之和array([12, 15, 18, 21])&gt;&gt;&gt;&gt;&gt;&gt; b.min(axis=1) # 每行最小值array([0, 4, 8])&gt;&gt;&gt;&gt;&gt;&gt; b.cumsum(axis=1) # 各行累加array([[ 0, 1, 3, 6], [ 4, 9, 15, 22], [ 8, 17, 27, 38]]) 通用数学函数12345678910&gt;&gt;&gt; B = np.arange(3)&gt;&gt;&gt; Barray([0, 1, 2])&gt;&gt;&gt; np.exp(B)array([ 1. , 2.71828183, 7.3890561 ])&gt;&gt;&gt; np.sqrt(B)array([ 0. , 1. , 1.41421356])&gt;&gt;&gt; C = np.array([2., -1., 4.])&gt;&gt;&gt; np.add(B, C)array([ 2., 0., 6.]) 索引，切片和迭代12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&gt;&gt;&gt; a = np.arange(10)**3&gt;&gt;&gt; aarray([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729])&gt;&gt;&gt; a[2]8&gt;&gt;&gt; a[2:5]array([ 8, 27, 64])&gt;&gt;&gt; a[:6:2] = -1000 # equivalent to a[0:6:2] = -1000; from start to position 6, exclusive, set every 2nd element to -1000&gt;&gt;&gt; aarray([-1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729])&gt;&gt;&gt; a[ : :-1] # reversed aarray([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000])&gt;&gt;&gt; for i in a:... print(i**(1/3.))...nan1.0nan3.0nan5.06.07.08.09.0&gt;&gt;&gt; def f(x,y):... return 10*x+y...&gt;&gt;&gt; b = np.fromfunction(f,(5,4),dtype=int)&gt;&gt;&gt; barray([[ 0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]])&gt;&gt;&gt; b[2,3]23&gt;&gt;&gt; b[0:5, 1] # 1到5行第二个array([ 1, 11, 21, 31, 41])&gt;&gt;&gt; b[ : ,1] # 每行第二个array([ 1, 11, 21, 31, 41])&gt;&gt;&gt; b[1:3, : ] # 2到3行array([[10, 11, 12, 13], [20, 21, 22, 23]])&gt;&gt;&gt; b[-1] # 最后一行array([40, 41, 42, 43])&gt;&gt;&gt; c = np.array( [[[ 0, 1, 2], # 3D数组... [ 10, 12, 13]],... [[100,101,102],... [110,112,113]]])&gt;&gt;&gt; c.shape(2, 2, 3)&gt;&gt;&gt; c[1,...] # 同 c[1,:,:] 和 c[1]array([[100, 101, 102], [110, 112, 113]])&gt;&gt;&gt; c[...,2] # 同 c[:,:,2]array([[ 2, 13], [102, 113]])&gt;&gt;&gt; for row in b:... print(row)...[0 1 2 3][10 11 12 13][20 21 22 23][30 31 32 33][40 41 42 43]&gt;&gt;&gt; for element in b.flat:... print(element)...012310111213202122233031323340414243 矩阵处理12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; a = np.floor(10*np.random.random((3,4)))&gt;&gt;&gt; aarray([[ 2., 8., 0., 6.], [ 4., 5., 1., 1.], [ 8., 9., 3., 6.]])&gt;&gt;&gt; a.shape(3, 4)&gt;&gt;&gt; a.ravel() # 返回扁平化的矩阵array([ 2., 8., 0., 6., 4., 5., 1., 1., 8., 9., 3., 6.])&gt;&gt;&gt; a.reshape(6,2) # 改变矩阵的形状array([[ 2., 8.], [ 0., 6.], [ 4., 5.], [ 1., 1.], [ 8., 9.], [ 3., 6.]])&gt;&gt;&gt; a.T # 矩阵的转置array([[ 2., 4., 8.], [ 8., 5., 9.], [ 0., 1., 3.], [ 6., 1., 6.]])&gt;&gt;&gt; a.T.shape(4, 3)&gt;&gt;&gt; a.shape(3, 4)&gt;&gt;&gt; aarray([[ 2., 8., 0., 6.], [ 4., 5., 1., 1.], [ 8., 9., 3., 6.]])&gt;&gt;&gt; a.resize((2,6))&gt;&gt;&gt; aarray([[ 2., 8., 0., 6., 4., 5.], [ 1., 1., 8., 9., 3., 6.]])&gt;&gt;&gt; a.reshape(3,-1)array([[ 2., 8., 0., 6.], [ 4., 5., 1., 1.], [ 8., 9., 3., 6.]]) 数组的分割1234567891011121314&gt;&gt;&gt; a = np.floor(10*np.random.random((2,12)))&gt;&gt;&gt; aarray([[ 9., 5., 6., 3., 6., 8., 0., 7., 9., 7., 2., 7.], [ 1., 4., 9., 2., 2., 1., 0., 6., 2., 2., 4., 0.]])&gt;&gt;&gt; np.hsplit(a,3)[array([[ 9., 5., 6., 3.], [ 1., 4., 9., 2.]]), array([[ 6., 8., 0., 7.], [ 2., 1., 0., 6.]]), array([[ 9., 7., 2., 7.], [ 2., 2., 4., 0.]])]&gt;&gt;&gt; np.hsplit(a,(3,4))[array([[ 9., 5., 6.], [ 1., 4., 9.]]), array([[ 3.], [ 2.]]), array([[ 6., 8., 0., 7., 9., 7., 2., 7.], [ 2., 1., 0., 6., 2., 2., 4., 0.]])] 复制1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; a = np.arange(12)&gt;&gt;&gt; b = a # 并没有创建新数组&gt;&gt;&gt; b is aTrue&gt;&gt;&gt; b.shape = 3,4&gt;&gt;&gt; a.shape(3, 4)&gt;&gt;&gt; def f(x):... print(id(x))...&gt;&gt;&gt; id(a)148293216&gt;&gt;&gt; f(a)148293216&gt;&gt;&gt; c = a.view()&gt;&gt;&gt; c is aFalse&gt;&gt;&gt; c.base is aTrue&gt;&gt;&gt; c.flags.owndataFalse&gt;&gt;&gt;&gt;&gt;&gt; c.shape = 2,6 # a的形状不变&gt;&gt;&gt; a.shape(3, 4)&gt;&gt;&gt; c[0,4] = 1234 # a的数据会变&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; s = a[ : , 1:3] # 广播&gt;&gt;&gt; s[:] = 10&gt;&gt;&gt; aarray([[ 0, 10, 10, 3], [1234, 10, 10, 7], [ 8, 10, 10, 11]])&gt;&gt;&gt; d = a.copy() # 深复制&gt;&gt;&gt; d is aFalse&gt;&gt;&gt; d.base is aFalse&gt;&gt;&gt; d[0,0] = 9999&gt;&gt;&gt; aarray([[ 0, 10, 10, 3], [1234, 10, 10, 7], [ 8, 10, 10, 11]]) 索引技巧123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168&gt;&gt;&gt; a = np.arange(12)**2 # 平方array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121], dtype=int32)&gt;&gt;&gt; i = np.array( [ 1,1,3,8,5 ] )&gt;&gt;&gt; a[i] # 对应位置元素array([ 1, 1, 9, 64, 25])&gt;&gt;&gt;&gt;&gt;&gt; j = np.array( [ [ 3, 4], [ 9, 7 ] ] )&gt;&gt;&gt; a[j] # 对应位置元素array([[ 9, 16], [81, 49]])&gt;&gt;&gt; palette = np.array( [ [0,0,0], # black... [255,0,0], # red... [0,255,0], # green... [0,0,255], # blue... [255,255,255] ] ) # white&gt;&gt;&gt; image = np.array( [ [ 0, 1, 2, 0 ],... [ 0, 3, 4, 0 ] ] )&gt;&gt;&gt; palette[image]array([[[ 0, 0, 0], [255, 0, 0], [ 0, 255, 0], [ 0, 0, 0]], [[ 0, 0, 0], [ 0, 0, 255], [255, 255, 255], [ 0, 0, 0]]])&gt;&gt;&gt; a = np.arange(12).reshape(3,4)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; i = np.array( [ [0,1],... [1,2] ] )&gt;&gt;&gt; j = np.array( [ [2,1],... [3,3] ] )&gt;&gt;&gt; a[i]array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 4, 5, 6, 7], [ 8, 9, 10, 11]]])&gt;&gt;&gt; a[i,j]array([[ 2, 5], [ 7, 11]])&gt;&gt;&gt;&gt;&gt;&gt; a[i,2]array([[ 2, 6], [ 6, 10]])&gt;&gt;&gt;&gt;&gt;&gt; a[:,j]array([[[ 2, 1], [ 3, 3]], [[ 6, 5], [ 7, 7]], [[10, 9], [11, 11]]])&gt;&gt;&gt; l = [i,j]&gt;&gt;&gt; a[l]array([[ 2, 5], [ 7, 11]])&gt;&gt;&gt; s = np.array( [i,j] )&gt;&gt;&gt; a[s]Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in ?IndexError: index (3) out of range (0&lt;=index&lt;=2) in dimension 0&gt;&gt;&gt;&gt;&gt;&gt; a[tuple(s)] # 同 a[i,j]array([[ 2, 5], [ 7, 11]])&gt;&gt;&gt; time = np.linspace(20, 145, 5)&gt;&gt;&gt; data = np.sin(np.arange(20)).reshape(5,4)&gt;&gt;&gt; timearray([ 20. , 51.25, 82.5 , 113.75, 145. ])&gt;&gt;&gt; dataarray([[ 0. , 0.84147098, 0.90929743, 0.14112001], [-0.7568025 , -0.95892427, -0.2794155 , 0.6569866 ], [ 0.98935825, 0.41211849, -0.54402111, -0.99999021], [-0.53657292, 0.42016704, 0.99060736, 0.65028784], [-0.28790332, -0.96139749, -0.75098725, 0.14987721]])&gt;&gt;&gt;&gt;&gt;&gt; ind = data.argmax(axis=0) # 各行最大值索引&gt;&gt;&gt; indarray([2, 0, 3, 1])&gt;&gt;&gt;&gt;&gt;&gt; time_max = time[ind]&gt;&gt;&gt;&gt;&gt;&gt; data_max = data[ind, range(data.shape[1])]&gt;&gt;&gt;&gt;&gt;&gt; time_maxarray([ 82.5 , 20. , 113.75, 51.25])&gt;&gt;&gt; data_maxarray([ 0.98935825, 0.84147098, 0.99060736, 0.6569866 ])&gt;&gt;&gt;&gt;&gt;&gt; np.all(data_max == data.max(axis=0))True&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; aarray([0, 1, 2, 3, 4])&gt;&gt;&gt; a[[1,3,4]] = 0&gt;&gt;&gt; aarray([0, 0, 2, 0, 0])&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; a[[0,0,2]]=[1,2,3]&gt;&gt;&gt; aarray([2, 1, 3, 3, 4])&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; a[[0,0,2]]+=1&gt;&gt;&gt; aarray([1, 1, 3, 3, 4])&gt;&gt;&gt; a = np.arange(12).reshape(3,4)&gt;&gt;&gt; b = a &gt; 4&gt;&gt;&gt; barray([[False, False, False, False], [False, True, True, True], [ True, True, True, True]])&gt;&gt;&gt; a[b]array([ 5, 6, 7, 8, 9, 10, 11])&gt;&gt;&gt; a[b] = 0 # 大于4均变成0&gt;&gt;&gt; aarray([[0, 1, 2, 3], [4, 0, 0, 0], [0, 0, 0, 0]])&gt;&gt;&gt; a = np.arange(12).reshape(3,4)&gt;&gt;&gt; b1 = np.array([False,True,True]) # first dim selection&gt;&gt;&gt; b2 = np.array([True,False,True,False]) # second dim selection&gt;&gt;&gt;&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt;&gt;&gt;&gt; a[b1,:] # 选择行array([[ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt;&gt;&gt;&gt; a[b1] # 同上array([[ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt;&gt;&gt;&gt; a[:,b2] # 选择列array([[ 0, 2], [ 4, 6], [ 8, 10]])&gt;&gt;&gt;&gt;&gt;&gt; a[b1,b2] # 很奇怪的选择array([ 4, 10]) 曼德布洛特集合 1234567891011121314151617import numpy as npimport matplotlib.pyplot as pltdef mandelbrot( h,w, maxit=20 ): \"\"\"Returns an image of the Mandelbrot fractal of size (h,w).\"\"\" y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ] c = x+y*1j z = c divtime = maxit + np.zeros(z.shape, dtype=int) for i in range(maxit): z = z**2 + c diverge = z*np.conj(z) &gt; 2**2 div_now = diverge &amp; (divtime==maxit) divtime[div_now] = i z[diverge] = 2 return divtimeplt.imshow(mandelbrot(400,400))plt.show() 线性代数1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.array([[1.0, 2.0], [3.0, 4.0]])&gt;&gt;&gt; print(a)[[ 1. 2.] [ 3. 4.]]&gt;&gt;&gt; a.transpose()array([[ 1., 3.], [ 2., 4.]])&gt;&gt;&gt; np.linalg.inv(a)array([[-2. , 1. ], [ 1.5, -0.5]])&gt;&gt;&gt; u = np.eye(2) # 2x2 单位矩阵; \"eye\" 表示 \"I\"，单位矩阵&gt;&gt;&gt; uarray([[ 1., 0.], [ 0., 1.]])&gt;&gt;&gt; j = np.array([[0.0, -1.0], [1.0, 0.0]])&gt;&gt;&gt; j @ j # 矩阵array([[-1., 0.], [ 0., -1.]])&gt;&gt;&gt; np.trace(u) # 计算对角线元素的和2.0&gt;&gt;&gt; y = np.array([[5.], [7.]])&gt;&gt;&gt; np.linalg.solve(a, y)array([[-3.], [ 4.]])&gt;&gt;&gt; np.linalg.eig(j)(array([ 0.+1.j, 0.-1.j]), array([[ 0.70710678+0.j , 0.70710678-0.j ], [ 0.00000000-0.70710678j, 0.00000000+0.70710678j]])) 小技巧“自动”变型123456789101112131415&gt;&gt;&gt; a = np.arange(30)&gt;&gt;&gt; a.shape = 2,-1,3 # -1 means \"whatever is needed\"&gt;&gt;&gt; a.shape(2, 5, 3)&gt;&gt;&gt; aarray([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]], [[15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29]]]) 处理直方图12345678&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; # 方差 0.5^2， 均值 2&gt;&gt;&gt; mu, sigma = 2, 0.5&gt;&gt;&gt; v = np.random.normal(mu,sigma,10000)&gt;&gt;&gt; # 标准直方图&gt;&gt;&gt; plt.hist(v, bins=50, density=1)&gt;&gt;&gt; plt.show() 1234&gt;&gt;&gt; # 使用numpy计算&gt;&gt;&gt; (n, bins) = np.histogram(v, bins=50, density=True)&gt;&gt;&gt; plt.plot(.5*(bins[1:]+bins[:-1]), n)&gt;&gt;&gt; plt.show()","categories":[],"tags":[{"name":"numpy","slug":"numpy","permalink":"https://oobspark.github.io/tags/numpy/"}]},{"title":"使用Python OpenCV提取图片中的特定物体","slug":"get-object-in-image-with-python-opencv","date":"2018-06-17T01:41:12.000Z","updated":"2019-11-12T07:32:27.582Z","comments":true,"path":"2018/06/17/get-object-in-image-with-python-opencv/","link":"","permalink":"https://oobspark.github.io/2018/06/17/get-object-in-image-with-python-opencv/","excerpt":"","text":"OpenCVOpenCV是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它轻量级而且高效——由一系列C函数和少量C++类构成，同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。 HSV颜色模型HSV（Hue, Saturation, Value）是根据颜色的直观特性由A. R. Smith在1978年创建的一种颜色空间, 也称六角锥体模型（Hexcone Model）。、这个模型中颜色的参数分别是：色调（H），饱和度（S），亮度（V）。 目前在计算机视觉领域存在着较多类型的颜色空间（color space）。HSV是其中一种最为常见的颜色模型，它重新影射了RGB模型，从而能够视觉上比RGB模型更具有视觉直观性。 一般对颜色空间的图像进行有效处理都是在HSV空间进行的，HSV的取值范围如下： 12345H: 0 ~ 180S: 0 ~ 255V: 0 ~ 255 目标 这是我们的原图，我们希望把图片中间的绿色区域“扣”出来。 代码示例源码地址image_cutter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#!/usr/bin/env pythonimport cv2import numpy as npdef find_center_point(file, blue_green_red=[], target_range=(), DEBUG=False): result = False if not blue_green_red: return result # 偏移量 thresh = 30 hsv = cv2.cvtColor(np.uint8([[blue_green_red]]), cv2.COLOR_BGR2HSV)[0][0] lower = np.array([hsv[0] - thresh, hsv[1] - thresh, hsv[2] - thresh]) upper = np.array([hsv[0] + thresh, hsv[1] + thresh, hsv[2] + thresh]) # 载入图片 img = cv2.imread(file) # 获取图片HSV颜色空间 hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # 获取遮盖层 mask = cv2.inRange(hsv, lower, upper) # 模糊处理 blurred = cv2.blur(mask, (9, 9)) # 二进制化 ret,binary = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY) # 填充大空隙 kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 7)) closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel) # 填充小斑点 erode = cv2.erode(closed, None, iterations=4) dilate = cv2.dilate(erode, None, iterations=4) # 查找轮廓 _, contours, _ = cv2.findContours( dilate.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) i = 0 centers = [] for con in contours: # 轮廓转换为矩形 rect = cv2.minAreaRect(con) if not (target_range and rect[1][0] &gt;= target_range[0] - 5 and rect[1][0] &lt;= target_range[0] + 5 and rect[1][1] &gt;= target_range[1] - 5 and rect[1][1] &lt;= target_range[1] + 5): continue centers.append(rect) if DEBUG: # 矩形转换为box对象 box=np.int0(cv2.boxPoints(rect)) # 计算矩形的行列起始值 y_right = max([box][0][0][1], [box][0][1][1], [box][0][2][1], [box][0][3][1]) y_left = min([box][0][0][1], [box][0][1][1], [box][0][2][1], [box][0][3][1]) x_right = max([box][0][0][0], [box][0][1][0], [box][0][2][0], [box][0][3][0]) x_left = min([box][0][0][0], [box][0][1][0], [box][0][2][0], [box][0][3][0]) if y_right - y_left &gt; 0 and x_right - x_left &gt; 0: i += 1 # 裁剪目标矩形区域 target = img[y_left:y_right, x_left:x_right] target_file = 'target_&#123;&#125;'.format(str(i)) cv2.imwrite(target_file + '.png', target) cv2.imshow(target_file, target) print('rect: &#123;&#125;'.format(rect)) print('y: &#123;&#125;,&#123;&#125;'.format(y_left, y_right)) print('x: &#123;&#125;,&#123;&#125;'.format(x_left, x_right)) if DEBUG: cv2.imshow('origin', img) cv2.waitKey(0) cv2.destroyAllWindows() return centersif __name__ == '__main__': # 目标的 bgr 颜色值，请注意顺序 # 左边的绿色盒子 bgr = [40, 158, 31] # 右边的绿色盒子 # bgr = [40, 158, 31] point = find_center_point('opencv-sample-box.png', blue_green_red=bgr, DEBUG=True) # 中心坐标 # point: [((152.0, 152.0), (63.99999237060547, 61.99999237060547), -0.0)] print(point[0][0][0]) 运行之后我们得到了我们的目标图区域： 一般来说，我们会选择一些比较纯净的颜色区块，从而比较容易控制噪点，提高准确率。","categories":[],"tags":[{"name":"opencv","slug":"opencv","permalink":"https://oobspark.github.io/tags/opencv/"},{"name":"python","slug":"python","permalink":"https://oobspark.github.io/tags/python/"},{"name":"图像物体识别","slug":"图像物体识别","permalink":"https://oobspark.github.io/tags/图像物体识别/"}]},{"title":"AnyProxy的自定义规则","slug":"custom-rules-on-anyproxy","date":"2018-06-11T07:22:30.000Z","updated":"2019-11-12T07:32:27.580Z","comments":true,"path":"2018/06/11/custom-rules-on-anyproxy/","link":"","permalink":"https://oobspark.github.io/2018/06/11/custom-rules-on-anyproxy/","excerpt":"","text":"概述AnyProxy是一个开放式的HTTP代理服务器。 主要特性包括： 基于Node.js，开放二次开发能力，允许自定义请求处理逻辑 支持Https的解析 提供GUI界面，用以观察请求 类似的软件还有Fiddler，Charles等。对于二次开发能力的支持，Fiddler 提供脚本自定义功能（Fiddler Script）。 Fiddler Script的本质其实是用JScript.NET语言写的一个脚本文件CustomRules.js，语法类似于C#，通过修改CustomRules.js可以很容易的修改http的请求和应答，不用中断程序，还可以针对不同的URI做特殊的处理。 但是如果想要进行更加深入的定制则有些捉襟见肘了，例如发起调用远程API接口等。当然如果你是C#使用者，这当然不在话下了。 我们都知道Node.js几乎可以做差不多任何事:)，而基于Node.js的AnyProxy则给予了二次定制更大的空间。 安装因为是基于Node.js，故而Node支持的平台AnyProxy都能支持了。 npm install -g anyproxy 对于Debian或者Ubuntu系统，在安装AnyProxy之前，可能还需要安装 nodejs-legacy。 sudo apt-get install nodejs-legacy 启动 命令行启动AnyProxy，默认端口号8001 anyproxy 启动后将终端http代理服务器配置为127.0.0.1:8001即可 访问http://127.0.0.1:8002 ，web界面上能看到所有的请求信息 rule模块AnyProxy提供了二次开发的能力，你可以用js编写自己的规则模块（rule），来自定义网络请求的处理逻辑。 例如我们想针对某些域名做检测，看经过AnyProxy代理的请求中是否包含了我们想要检测的那些域名。那么我们可以通过以下脚本实现： 首先我们安装两个包 npm install redis npm install request 然后编写文件check.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// file: check.jsvar redis = require('redis')var request = require('request')var redisOn = truevar client = redis.createClient('6379', '127.0.0.1')client.on(\"error\", function(error) &#123; console.log(error); var redisOn = false&#125;)var domainsListToCheck = [ 'domainToCheck1', 'domainToCheck2', 'domainToCheck3', 'domainToCheck4', 'domainToCheck5',]module.exports = &#123; *beforeSendResponse(requestDetail, responseDetail) &#123; var inList = false for (var i = 0; i &lt; domainsListToCheck.length; i++) &#123; inList = requestDetail.url.search(domainsListToCheck[i]) != -1 if(inList)&#123; break &#125; &#125; if (inList) &#123; var ua = requestDetail.requestOptions.headers['User-Agent'].toLowerCase() var ourAgent = '' if(ua.search('iphone') != -1)&#123; ourAgent = 'iphone' &#125; if(ourAgent)&#123; if(redisOn)&#123; client.select('0', function(error)&#123; client.set(ourAgent, '1', function(error, res) &#123; console.log(error, res) &#125;) &#125;) &#125;else&#123; request(&#123; url: 'https://keyvalue.immanuel.co/api/KeyVal/UpdateValue/lglm4ov9/'+ourAgent+'/1', method: \"POST\", &#125;, function(error, response, body) &#123; console.log(error, response, body) &#125;); &#125; &#125; return null &#125; &#125;,&#125; 值得注意的是，我们在脚本中还是使用了一个本地Redis服务，如果你不想在本地启动一个Redis实例，你也可以使用keyvalue.immanuel.co。 keyvalue.immanuel.co是一个在线的Key-Value存储服务，完全免费。对于这种临时的，不重要的标记真是再方便不过了。个人使用下来觉得很赞。 使用自定义rule模块anyproxy --rule check.js 了解更多AnyProxy的更多功能可以参考官方文档。","categories":[],"tags":[{"name":"anyproxy","slug":"anyproxy","permalink":"https://oobspark.github.io/tags/anyproxy/"}]},{"title":"代理服务器的匿名级别","slug":"proxy-server","date":"2018-06-06T02:11:21.000Z","updated":"2019-11-12T07:32:27.586Z","comments":true,"path":"2018/06/06/proxy-server/","link":"","permalink":"https://oobspark.github.io/2018/06/06/proxy-server/","excerpt":"","text":"概述代理服务器（Proxy Server）的基本行为就是接收客户端发送的请求后转发给其他服务器。代理不改变请求URI，会直接发送给前方持有资源的目标服务器。根据代理类型的不同，我们对于目标服务器的匿名程度也有所不同。 未使用代理在没有经过代理服务器的情况下，目标服务端能获取到如下信息。 123REMOTE_ADDR = your IPHTTP_VIA = not determinedHTTP_X_FORWARDED_FOR = not determined 透明代理（Transparent Proxy）123REMOTE_ADDR = proxy IPHTTP_VIA = proxy IPHTTP_X_FORWARDED_FOR = your IP 透明代理虽然可以直接“隐藏”你的IP地址，但还是可以从HTTP_X_FORWARDED_FOR查到你是IP地址。这也是我们一般所说的Cache Proxy。 匿名代理（Anonymous Proxy）123REMOTE_ADDR = proxy IPHTTP_VIA = proxy IPHTTP_X_FORWARDED_FOR = proxy IP 使用匿名代理，别人只能知道你用了代理，无法知道你是谁。这也是使用得比较广泛的一种代理方式。 混淆代理（Distorting Proxy）123REMOTE_ADDR = proxy IPHTTP_VIA = proxy IPHTTP_X_FORWARDED_FOR = random IP address 使用了混淆代理，别人还是能知道你在用代理，但是会得到一个假的IP地址。 高匿代理（Elite proxy或High Anonymity Proxy）123REMOTE_ADDR = Proxy IPHTTP_VIA = not determinedHTTP_X_FORWARDED_FOR = not determined 使用高匿代理时，我们发现跟我们不使用代理是一样的，别人此时根本无法发现你是在用代理服务，这是最好的选择。","categories":[],"tags":[{"name":"代理服务器","slug":"代理服务器","permalink":"https://oobspark.github.io/tags/代理服务器/"},{"name":"Proxy","slug":"Proxy","permalink":"https://oobspark.github.io/tags/Proxy/"}]},{"title":"使用pac文件屏蔽那些不想访问的网站","slug":"pac-file-for-getting-away-from-specified-sites","date":"2018-06-05T03:13:41.000Z","updated":"2019-11-12T07:32:27.585Z","comments":true,"path":"2018/06/05/pac-file-for-getting-away-from-specified-sites/","link":"","permalink":"https://oobspark.github.io/2018/06/05/pac-file-for-getting-away-from-specified-sites/","excerpt":"","text":"概述代理自动配置（Proxy auto-config，简称PAC），用于定义应用该如何自动选择适当的代理服务器来访问一个网址。一般使用在浏览器中，现在在一些便携式设备（Android设备，iOS设备）中，WIFI连接时也可以添加PAC协议来实现自动代理配置。 使用场景在我们的场景中，我们有一些网站我们并不想去访问，譬如一些自动弹出的广告，某一个糟心的网站等等。这时就可以利用PAC文件来主动屏蔽这些网站。 使用方法 编写PAC文件filter.pac 12345678910111213141516171819202122232425262728// 我们不想访问的网站域名var FILTERS = [ &quot;do_not_want_to_visit_1.com&quot;, &quot;do_not_want_to_visit_2.com&quot;, &quot;do_not_want_to_visit_3.com&quot;,];// 一个没有在使用的端口，因为我们不想访问这些网站var PROXY = &quot;PROXY 127.0.0.1:9999&quot;;var DERECT = &quot;DIRECT&quot;;// PAC的主方法function FindProxyForURL(url, host) &#123; function rule_filter(domain) &#123; for (var i = 0; i &lt; FILTERS.length; i++) &#123; if (domain.search(FILTERS[i]) !== -1) &#123; return false; &#125; &#125; return true; &#125; if (rule_filter(host) === true) &#123; return DERECT; &#125; else &#123; return PROXY; &#125;&#125; 把PAC文件上传到网络上，且保证我们能直接访问到该文件。如果你有GitHub账号，你可以直接在你的项目中上传该文件。 在浏览器插件（如SwitchySharp）中，或者是在WIFI链接时的代理设置-&gt;自动选项中，填入我们上传的地址（我们的场景是https://github.com/oobspark/oobspark.github.io/blob/master/files/filter.pac）。","categories":[],"tags":[{"name":"pac","slug":"pac","permalink":"https://oobspark.github.io/tags/pac/"},{"name":"屏蔽网站","slug":"屏蔽网站","permalink":"https://oobspark.github.io/tags/屏蔽网站/"}]},{"title":"使用安卓ADB工具模拟滑动操作的两种简易方式","slug":"two-easy-ways-to-simulate-drag-gesture-with-android-adb","date":"2018-06-04T01:11:35.000Z","updated":"2019-11-12T07:32:27.587Z","comments":true,"path":"2018/06/04/two-easy-ways-to-simulate-drag-gesture-with-android-adb/","link":"","permalink":"https://oobspark.github.io/2018/06/04/two-easy-ways-to-simulate-drag-gesture-with-android-adb/","excerpt":"","text":"近日需要在安卓设备中模拟滑动操作，进行一番研究之后，发现了两种比较简易的方式。不能不感叹安卓Android Debug Bridge (adb)工具功能之强大。下面进入正文。 环境搭建 接入设备并安装设备驱动（此过程请自行百度） Windows系统中，从官网下载ADB Kits并解压，譬如解压为D:\\adb。 为了方便起见，一般我们可以把D:\\adb加入Windows环境变量中。此处我们直接由cmd进入adb目录 执行adb devices，这时候我们就能看到之前接入的设备标识，这就说明adb已经可以正常使用了 第一种方式（adb shell input swipe)adb shell input swipe xStart yStart xEnd yEnd duration 此方法比较简单，也是使用得比较广泛的一种，GOOGLE或BAIDU到的大量文章都是基于此方法实现的。 adb shell input swipe 200 600 200 300 1000，表示从坐标（200,600）这个点滑动到坐标（200,300），1000毫秒内完成。表现在屏幕上就是上滑过程。 第二种方式（Monkey Script)Drag(xStart, yStart, xEnd, yEnd, stepCount) Money命令是adb中用来测试程序稳定性的一个工具。根据参数的不同，它可以产生不同的测试效果。用它可以产生很多随机事件，当然，也可以使用Monkey Script来产生很多指定事件。Monkey Script，我们来了解一下。 Monkey也是属于adb工具的一部分，所以还是要先安装好adb，请参考文章第一部分 编写Monkey Script脚本，并命名为drag.mks 12345count = 1speed = 1.0start data &gt;&gt; Drag(200,600,200,300,100)UserWait(500) adb push drag.mks /data/local/ adb shell monkey -f /data/local/drag.mks 10，将上滑操作执行10次","categories":[],"tags":[{"name":"adb","slug":"adb","permalink":"https://oobspark.github.io/tags/adb/"},{"name":"andriod","slug":"andriod","permalink":"https://oobspark.github.io/tags/andriod/"},{"name":"simulate","slug":"simulate","permalink":"https://oobspark.github.io/tags/simulate/"},{"name":"gesture","slug":"gesture","permalink":"https://oobspark.github.io/tags/gesture/"},{"name":"drag","slug":"drag","permalink":"https://oobspark.github.io/tags/drag/"},{"name":"swipe","slug":"swipe","permalink":"https://oobspark.github.io/tags/swipe/"}]},{"title":"作为正向代理的Apache Traffic Server","slug":"apache-traffic-server-as-forward-proxy","date":"2018-05-31T07:14:35.000Z","updated":"2019-11-12T07:32:27.580Z","comments":true,"path":"2018/05/31/apache-traffic-server-as-forward-proxy/","link":"","permalink":"https://oobspark.github.io/2018/05/31/apache-traffic-server-as-forward-proxy/","excerpt":"","text":"对比 安装 环境：ubuntu 16.04 sudo apt-get install trafficserver 配置更改records.config配置，一般为/etc/trafficserver/records.config 123CONFIG proxy.config.url_remap.remap_required INT 0CONFIG proxy.config.http.cache.http INT 1CONFIG proxy.config.reverse_proxy.enabled INT 0 如果不想返回头中包含有ATS信息，如12Proxy-Connection: keep-aliveServer: ATS/3.2.4 可以参照以下配置 1234567CONFIG proxy.config.http.response_server_enabled INT 0CONFIG proxy.config.http.insert_age_in_response INT 0CONFIG proxy.config.http.insert_request_via_str INT 0 CONFIG proxy.config.http.anonymize_insert_client_ip INT 0 CONFIG proxy.config.http.insert_squid_x_forwarded_for INT 0 CONFIG proxy.config.reverse_proxy.enabled INT 0 CONFIG proxy.config.url_remap.remap_required INT 0 启动sudo service trafficserver restart 使用正向代理配置代理服务器：IP:8080，8080为默认端口","categories":[],"tags":[{"name":"Apache Traffic Server","slug":"Apache-Traffic-Server","permalink":"https://oobspark.github.io/tags/Apache-Traffic-Server/"},{"name":"正向代理","slug":"正向代理","permalink":"https://oobspark.github.io/tags/正向代理/"},{"name":"ForwardProxy","slug":"ForwardProxy","permalink":"https://oobspark.github.io/tags/ForwardProxy/"}]},{"title":"特征提取之词向量","slug":"feature-extraction-Word2Vec","date":"2018-05-22T03:34:30.000Z","updated":"2019-11-12T07:32:27.581Z","comments":true,"path":"2018/05/22/feature-extraction-Word2Vec/","link":"","permalink":"https://oobspark.github.io/2018/05/22/feature-extraction-Word2Vec/","excerpt":"","text":"特征提取-词向量Word2Vec（词向量），计算每个单词在其给定语料库环境下的分布式词向量（Distributed Representation）。 如果词的语义相近，那么它们的词向量在向量空间中也相互接近，这使得词语的向量化建模更加精确，可以改善现有方法并提高鲁棒性。词向量已经在许多自然语言处理场景中得到应用，如：命名实体识别，消歧，标注，解析，机器翻译等。 代码示例相关API ：Word2Vec 1234567891011121314151617from pyspark.ml.feature import Word2Vecspark = SparkSession.builder.master(\"local\").appName(\"Word2Vec\").getOrCreate()documentDF = spark.createDataFrame([ (\"Hi I heard about Spark\".split(\" \"), ), (\"I wish Java could use case classes\".split(\" \"), ), (\"Logistic regression models are neat\".split(\" \"), )], [\"text\"])word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")model = word2Vec.fit(documentDF)result = model.transform(documentDF)for row in result.collect(): text, vector = row print(\"Text: [%s] =&gt; \\nVector: %s\\n\" % (\", \".join(text), str(vector))) 参考文章","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"https://oobspark.github.io/tags/spark/"},{"name":"特征提取","slug":"特征提取","permalink":"https://oobspark.github.io/tags/特征提取/"},{"name":"Word2Vec","slug":"Word2Vec","permalink":"https://oobspark.github.io/tags/Word2Vec/"},{"name":"词向量","slug":"词向量","permalink":"https://oobspark.github.io/tags/词向量/"}]},{"title":"MySQL事务隔离级别","slug":"mysql-transaction-isolation-level","date":"2018-05-21T06:41:12.000Z","updated":"2019-11-12T07:32:27.584Z","comments":true,"path":"2018/05/21/mysql-transaction-isolation-level/","link":"","permalink":"https://oobspark.github.io/2018/05/21/mysql-transaction-isolation-level/","excerpt":"","text":"事务的基本要素（ACID） Atomicity（原子性）：一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性。 Consistency（一致性）：数据库总是从一个一致性状态转换到另一个一致状态。 Isolation（隔离性）：通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。注意这里的“通常来说”，后面的事务隔离级级别会说到。 Durability（持久性）：一旦事务提交，则其所做的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。（持久性的安全性与刷新日志级别也存在一定关系，不同的级别对应不同的数据安全级别。） MySQL事务隔离级别SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 Read Uncommitted（未提交读）：在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（提交读）：这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。 Repeatable Read（可重复读）：这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 Serializable（可串行化）：这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 事务的并发问题 脏读（Dirty Read）：最容易理解。另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据。 不重复读（NonRepeatable Read）：解决了脏读后，会遇到，同一个事务执行过程中，另外一个事务提交了新数据，因此本事务先后两次读到的数据结果会不一致。 幻读（Phantom Read）：解决了不重复读，保证了同一个事务里，查询的结果都是事务开始并且第一次查询时的状态（一致性）。但是，如果另一个事务同时提交了新数据，虽然本事务再次按照相同的条件查找会得到相同的结果集，但是本事务指定更新时(看了后面的演示你就会知道)，就会“惊奇的”发现了这些新数据，貌似之前读到的数据是“鬼影”一样的幻觉。 不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 隔离级别 脏读Dirty Read 不可重复读NonRepeatable Read 幻读Phantom Read 未提交读Read uncommitted 可能 可能 可能 已提交读Read committed 不可能 可能 可能 可重复读Repeatable read 不可能 不可能 可能 可串行化Serializable 不可能 不可能 不可能","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://oobspark.github.io/tags/MySQL/"},{"name":"事务","slug":"事务","permalink":"https://oobspark.github.io/tags/事务/"},{"name":"事务隔离级别","slug":"事务隔离级别","permalink":"https://oobspark.github.io/tags/事务隔离级别/"}]},{"title":"特征提取之TF-IDF算法","slug":"feature-extraction-TF-IDF","date":"2018-05-14T06:41:12.000Z","updated":"2019-11-12T07:32:27.581Z","comments":true,"path":"2018/05/14/feature-extraction-TF-IDF/","link":"","permalink":"https://oobspark.github.io/2018/05/14/feature-extraction-TF-IDF/","excerpt":"","text":"特征提取之TF-IDFTF-IDF（词频-逆文档频率）算法是一种统计方法，用以评估一字词对于一个文件集或者一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上就是 TF*IDF，其中 TF（Term Frequency），表示词条在文章Document 中出现的频率；IDF（Inverse Document Frequency），其主要思想就是，如果包含某个词 Word的文档越少，则这个词的区分度就越大，也就是 IDF 越大。对于如何获取一篇文章的关键词，我们可以计算这边文章出现的所有名词的 TF-IDF，TF-IDF越大，则说明这个名词对这篇文章的区分度就越高，取 TF-IDF 值较大的几个词，就可以当做这篇文章的关键词。 在此推荐这篇文章。 计算步骤 计算词频（TF） $$词频={某个词在文章中的出现次数 \\over 文章总次数}$$ 计算逆文档频率（IDF） $$逆文档频率 = log {语料库的文档总数 \\over 文章总次数}$$ 计算词频-逆文档频率（TF-IDF） $$词频-逆文档频率 = 词频 * 逆文档频率$$ 代码示例相关API ：HashingTF，IDF 123456789101112131415161718192021from pyspark.ml.feature import HashingTF, IDF, Tokenizerspark = SparkSession.builder.master(\"local\").appName(\"TF-IDF\").getOrCreate()sentenceData = spark.createDataFrame([ (0.0, \"Hi I heard about Spark\"), (0.0, \"I wish Java could use case classes\"), (1.0, \"Logistic regression models are neat\")], [\"label\", \"sentence\"])tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")wordsData = tokenizer.transform(sentenceData)hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)featurizedData = hashingTF.transform(wordsData)idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")idfModel = idf.fit(featurizedData)rescaledData = idfModel.transform(featurizedData)rescaledData.select(\"label\", \"features\").show() 可参考examples/src/main/python/ml/word2vec_example.py","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"https://oobspark.github.io/tags/spark/"},{"name":"TF-IDF","slug":"TF-IDF","permalink":"https://oobspark.github.io/tags/TF-IDF/"},{"name":"特征提取","slug":"特征提取","permalink":"https://oobspark.github.io/tags/特征提取/"}]},{"title":"精读深度学习-前言","slug":"Deep-Learning-Chapter-1","date":"2018-04-26T03:34:54.000Z","updated":"2019-11-12T07:32:27.578Z","comments":true,"path":"2018/04/26/Deep-Learning-Chapter-1/","link":"","permalink":"https://oobspark.github.io/2018/04/26/Deep-Learning-Chapter-1/","excerpt":"","text":"前言AI 系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。这种能力被称为机器学习（machine learning）。 引入机器学习使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决策。比如，一个被称为逻辑回归（logistic regression）的简单机器学习算法可以决定是否建议剖腹产 （Mor-Yosef et al., 1990）。而同样是简单机器学习算法的朴素贝叶斯（naive Bayes）则可以区分垃圾电子邮件和合法电子邮件。 这些简单的机器学习算法的性能在很大程度上依赖于给定数据的表示（repre-sentation）。 然而，对于许多任务来说，我们很难知道应该提取哪些特征。例如，假设我们想编写一个程序来检测照片中的车。我们知道，汽车有轮子，所以我们可能会想用车轮的存在与否作为特征。不幸的是，我们难以准确地根据像素值来描述车轮看上去像什么。虽然车轮具有简单的几何形状，但它的图像可能会因场景而异，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体等等。 解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为表示学习（representation learning）。 在许多现实的人工智能应用中，困难主要源于多个变差因素同时影响着我们能够观察到的每一个数据。比如，在一张包含红色汽车的图片中，其单个像素在夜间可能会非常接近黑色。汽车轮廓的形状取决于视角。大多数应用需要我们理清变差因素并忽略我们不关心的因素。 深度学习（deep learning）通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。 深度学习模型的典型例子是前馈深度网络或多层感知机（multilayer perceptron, MLP）。多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。 深度学习是一种表示学习，也是一种机器学习，可以用于许多（但不是全部）AI 方法。 深度学习的进步也严重依赖于软件基础架构的进展。软件库如 Theano （Bergstra et al., 2010a; Bastien et al., 2012a）、PyLearn2 （Goodfellow et al., 2013e）、Torch （Col-lobert et al., 2011b）、DistBelief （Dean et al., 2012）、Caffe （Jia, 2013）、MXNet （Chen et al., 2015） 和 TensorFlow （Abadi et al., 2015） 都能支持重要的研究项目或商业产品。 总之，深度学习是机器学习的一种方法。在过去几十年的发展中，它大量借鉴了我们关于人脑、统计学和应用数学的知识。近年来，得益于更强大的计算机、更大的数据集和能够训练更深网络的技术，深度学习的普及性和实用性都有了极大的发展。未来几年充满了进一步提高深度学习并将它带到新领域的挑战和机遇。 Ref 深度学习（Deep Learning） 英文版地址http://www.deeplearningbook.org/","categories":[],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://oobspark.github.io/tags/Deep-Learning/"},{"name":"DL","slug":"DL","permalink":"https://oobspark.github.io/tags/DL/"},{"name":"前言","slug":"前言","permalink":"https://oobspark.github.io/tags/前言/"},{"name":"introduction","slug":"introduction","permalink":"https://oobspark.github.io/tags/introduction/"}]},{"title":"分布式数据仓库Hive","slug":"hive-installation","date":"2018-04-25T07:11:43.000Z","updated":"2019-11-12T07:32:27.583Z","comments":true,"path":"2018/04/25/hive-installation/","link":"","permalink":"https://oobspark.github.io/2018/04/25/hive-installation/","excerpt":"","text":"简介Hive是基于Hadoop的数据仓库解决方案。由于Hadoop本身在数据存储和计算方面有很好的可扩展性和高容错性，因此使用Hive构建的数据仓库也秉承了这些特性。 简单来说，Hive就是在Hadoop上架了一层SQL接口，可以将SQL翻译成MapReduce去Hadoop上执行，这样就使得数据开发和分析人员很方便的使用SQL来完成海量数据的统计和分析，而不必使用编程语言开发MapReduce那么麻烦。 环境依赖 Java Hadoop，可移步至Hadoop安装 Java 1.8 sudo apt-get update sudo apt-get install -y default-jre default-jdk Hive 2.3.3 wget http://mirrors.hust.edu.cn/apache/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz tar zxvf apache-hive-2.3.3-bin.tar.gz sudo mv apache-hive-2.3.3-bin /usr/local/hadoop/hive sudo chown vagrant:vagrant /usr/local/hadoop/hive PATHvi ~/.bashrc，添加以下代码1234export JAVA_HOME=/usr/lib/jvm/default-javaexport HIVE_HOME=/usr/local/hadoop/hiveexport PATH=$HIVE_HOME/bin:$PATHexport HADOOP_HOME=&lt;hadoop-install-dir&gt; HDFS上的文件初始化 $HADOOP_HOME/bin/hdfs dfs -mkdir /tmp $HADOOP_HOME/bin/hdfs dfs -mkdir /user/hive/warehouse $HADOOP_HOME/bin/hdfs dfs -chmod g+w /tmp $HADOOP_HOME/bin/hdfs dfs -chmod g+w /user/hive/warehouse Hive元数据的储存模式+“单用户”模式：利用该模式连接到内存数据库Derby，Derby不支持多会话连接，这种模式一般用于单机测试+“多用户”模式：通过网络和JDBC连接到规模更大的专业数据库，如MySQL+“远程服务器”模式：用于非Java客户端访问在远程服务器上储存的元数据库，需要在服务端启动一个MetaStoreServer，然后在客户端通过Thrift协议访问该服务器，进而访问到元数据 单用户模式 配置文件cp conf/hive-default.xml.template conf/hive-default.xml Hive Shell：bin/hive Hive集成MySQL数据库 新建hive数据库，用来保存hive的元数据mysql&gt; create database hive; 将hive数据库下的所有表的所有权限赋给hadoop用户，并配置mysql为hive-site.xml中的连接密码，然后刷新系统权限关系表 123mysql&gt; CREATE USER &apos;hadoop&apos;@&apos;%&apos; IDENTIFIED BY &apos;mysql&apos;;mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;hadoop&apos;@&apos;%&apos; WITH GRANT OPTION;mysql&gt; flush privileges; 多用户模式安装 vi conf/hbase-site.xml，没有的话新建 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;metadata is stored in a MySQL server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;MySQL JDBC driver class&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;user name for connecting to mysql server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;password for connecting to mysql server&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 下载相应的mysql jar包，解压后放入$HIVE_HIME/lib下lib/mysql-connector-java-5.1.46-bin.jar 初始化数据表schematool -dbType mysql -initSchemaschematool -dbType mysql -info Hive Shell： bin/hive 12345678910CREATE TABLE pokes (foo INT, bar STRING);CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);SHOW TABLES;SHOW TABLES &apos;.*s&apos;;DESCRIBE invites;LOAD DATA LOCAL INPATH &apos;./examples/files/kv1.txt&apos; OVERWRITE INTO TABLE pokes;LOAD DATA LOCAL INPATH &apos;./examples/files/kv2.txt&apos; OVERWRITE INTO TABLE invites PARTITION (ds=&apos;2008-08-15&apos;);LOAD DATA LOCAL INPATH &apos;./examples/files/kv3.txt&apos; OVERWRITE INTO TABLE invites PARTITION (ds=&apos;2008-08-08&apos;);SELECT a.foo FROM invites a WHERE a.ds=&apos;2008-08-15&apos;;INSERT OVERWRITE DIRECTORY &apos;/tmp/hdfs_out&apos; SELECT a.* FROM invites a WHERE a.ds=&apos;2008-08-15&apos;; DDL 和 DML","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://oobspark.github.io/tags/Hadoop/"},{"name":"Hive","slug":"Hive","permalink":"https://oobspark.github.io/tags/Hive/"},{"name":"数据仓库","slug":"数据仓库","permalink":"https://oobspark.github.io/tags/数据仓库/"}]},{"title":"分布式数据库HBase","slug":"hbase-installation","date":"2018-04-23T07:11:43.000Z","updated":"2019-11-12T07:32:27.582Z","comments":true,"path":"2018/04/23/hbase-installation/","link":"","permalink":"https://oobspark.github.io/2018/04/23/hbase-installation/","excerpt":"","text":"简介HBase – Hadoop Database，是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。 HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。 HBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。 环境依赖 Java，具体可参考推荐版本 ssh，sshd服务 Java 1.8 sudo apt-get update sudo apt-get install -y default-jre default-jdk HBase 1.3.2 wget http://mirrors.hust.edu.cn/apache/hbase/1.3.2/hbase-1.3.2-bin.tar.gz tar zxvf hbase-1.3.2-bin.tar.gz sudo mv hbase-1.3.2 /usr/local/hadoop/hbase sudo chown vagrant:vagrant /usr/local/hadoop，注意此处，必须为hbase上级目录设置权限 PATH vi ~/.bashrc，添加以下代码 12export JAVA_HOME=/usr/lib/jvm/default-javaexport HBASE_HOME=/usr/local/hadoop/hbase 单机模式安装 配置文件 hbase-site.xml： 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///opt/data/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/data/zookeeper&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; conf/hbase-env.sh文件中，修改：export JAVA_HOME=/usr/lib/jvm/default-javaexport HBASE_MANAGES_ZK=true，使用hbase自带zookeeper 运行： 启动服务bin/start-hbase.sh 停止服务bin/stop-hbase.sh 伪分布式安装 配置文件 hbase-site.xml： 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/data/zookeeper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; conf/hbase-env.sh文件中，修改：export JAVA_HOME=/usr/lib/jvm/default-javaexport HBASE_MANAGES_ZK=true，使用hbase自带zookeeper 运行： 启动服务bin/start-hbase.sh master副本bin/local-master-backup.sh start 2 3 5 region server副本bin/local-regionservers.sh start 2 3 停止服务bin/stop-hbase.shcat /tmp/hbase-vagrant-1-master.pid |xargs kill -9bin/local-regionservers.sh stop 3 HBase基本操作 连接到HBase 12$ ./bin/hbase shellhbase(main):001:0&gt; 显示帮助，help 建表 12hbase(main):001:0&gt; create &apos;test&apos;, &apos;cf&apos;0 row(s) in 0.4170 seconds 查看表信息 1234hbase(main):002:0&gt; list &apos;test&apos;TABLEtest1 row(s) in 0.0180 seconds 写数据 12345678hbase(main):003:0&gt; put &apos;test&apos;, &apos;row1&apos;, &apos;cf:a&apos;, &apos;value1&apos;0 row(s) in 0.0850 secondshbase(main):004:0&gt; put &apos;test&apos;, &apos;row2&apos;, &apos;cf:b&apos;, &apos;value2&apos;0 row(s) in 0.0110 secondshbase(main):005:0&gt; put &apos;test&apos;, &apos;row3&apos;, &apos;cf:c&apos;, &apos;value3&apos;0 row(s) in 0.0100 seconds 获取数据 123456hbase(main):006:0&gt; scan &apos;test&apos;ROW COLUMN+CELL row1 column=cf:a, timestamp=1421762485768, value=value1 row2 column=cf:b, timestamp=1421762491785, value=value2 row3 column=cf:c, timestamp=1421762496210, value=value33 row(s) in 0.0230 seconds 获取单行数据 1234hbase(main):007:0&gt; get &apos;test&apos;, &apos;row1&apos;COLUMN CELL cf:a timestamp=1421762485768, value=value11 row(s) in 0.0350 seconds 禁用表 12345hbase(main):008:0&gt; disable &apos;test&apos;0 row(s) in 1.1820 secondshbase(main):009:0&gt; enable &apos;test&apos;0 row(s) in 0.1770 seconds 删除表 12hbase(main):010:0&gt; drop &apos;test&apos;0 row(s) in 0.1370 seconds 退出 1hbase(main):011:0&gt; exit","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://oobspark.github.io/tags/Hadoop/"},{"name":"Hbase","slug":"Hbase","permalink":"https://oobspark.github.io/tags/Hbase/"}]},{"title":"Hadoop安装","slug":"hadoop-installation","date":"2018-04-21T05:11:43.000Z","updated":"2019-11-12T07:32:27.582Z","comments":true,"path":"2018/04/21/hadoop-installation/","link":"","permalink":"https://oobspark.github.io/2018/04/21/hadoop-installation/","excerpt":"","text":"环境依赖 Java，具体可参考推荐版本 ssh，sshd服务 Java 1.8 sudo apt-get update sudo apt-get install -y default-jre default-jdk Hadoop 2.7.5 wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.5/hadoop-2.7.5.tar.gz tar zxvf hadoop-2.7.5.tar.gz sudo mv hadoop-2.7.5 /usr/local/hadoop sudo chown vagrant /usr/local/hadoop PATH vi ~/.bashrc，添加以下代码 12export JAVA_HOME=/usr/lib/jvm/default-javaexport HADOOP_HOME=/usr/local/hadoop Hadoop目前支持的集群模式 单机模式安装 伪分布式模式安装 完全分布式安装 单机模式安装 mkdir input cp etc/hadoop/*.xml input bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar grep input output &#39;dfs[a-z.]+&#39; ls output 伪分布式模式安装 shh配置 ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys 测试：ssh localhost 配置文件 etc/hadoop/core-site.xml，ubuntu-xenial为服务器主机名或IP： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ubuntu-xenial:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hadoop-env.sh文件中，修改：export JAVA_HOME=/usr/lib/jvm/default-java 运行： 格式化文件系统bin/hdfs namenode -format 运行NameNode和DataNode服务sbin/start-dfs.sh 运行日志默认写在$HADOOP_LOG_DIR文件夹下，默认为$HADOOP_HOME/logs NameNode web界面http://ubuntu-xenial:50070/ 建立分布式文件夹bin/hdfs dfs -mkdir /userbin/hdfs dfs -mkdir /user/&lt;username&gt; 将本地文件复制到分布式系统bin/hdfs dfs -put etc/hadoop input 运行MapReduce任务bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar grep input output &#39;dfs[a-z.]+&#39; 从分布式文件系统文件复制到本地，并验证bin/hdfs dfs -get output outputcat output/*orbin/hdfs dfs -cat output/* 停止服务sbin/stop-dfs.sh YARN服务的引入 配置文件 etc/hadoop/mapred-site.xml： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/yarn-site.xml： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 运行YARN的ResourceManager和NodeManagersbin/start-yarn.sh ResourceManager的web界面http://ubuntu-xenial:8088/ 运行MapReduce任务 停止YARN服务sbin/stop-yarn.sh","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://oobspark.github.io/tags/Hadoop/"}]},{"title":"ML工作流（Pipelines）","slug":"ML-Pipelines","date":"2018-04-12T07:28:47.000Z","updated":"2019-11-12T07:32:27.579Z","comments":true,"path":"2018/04/12/ML-Pipelines/","link":"","permalink":"https://oobspark.github.io/2018/04/12/ML-Pipelines/","excerpt":"","text":"ML工作流（Pipelines）中的一些概念 DataFrame：使用Spark SQL中的DataFrame作为数据集，它可以容纳各种数据类型。 DataFrame中的列可以是存储的文本，特征向量，真实标签和预测标签等。 Transformer：转换器，是一种可以将一个DataFrame转换为另一个DataFrame的算法。比如一个模型就是一个Transformer。 Estimator：评估器，基于算法实现了一个fit()方法进行拟合，输入一个DataFrame，产生一个Transformer。 PipeLine：管道将多个工作流阶段（转换器和估计器）连接在一起，形成机器学习的工作流。 Parameter：用来设置所有转换器和估计器的参数。 Pipelines如何运转一个工作流被指定为一系列的阶段，每个阶段都是Transformer或Estimator。这些阶段按顺序运行，输入的DataFrame在通过每个阶段时会进行转换。对于Transformer阶段，会在DataFrame上调用transform()方法。对于Estimator阶段，调用fit()方法来拟合生成Transformer（它将成为PipelineModel或拟合管道的一部分），并在DataFrame上调用Transformer的transform()方法。 上图中，顶行表示具有三个阶段的管道。前两个（Tokenizer和HashingTF）是Transformers（蓝色），第三个（LogisticRegression）是Estimator（红色）。底行表示流经管道的数据，其中圆柱表示DataFrames。在原始DataFrame上调用Pipeline.fit()方法拟合，它具有原始的文本和标签。Tokenizer.transform()方法将原始文本拆分为单词，并向DataFrame添加一个带有单词的新列。 HashingTF.transform()方法将字列转换为特征向量，向这些向量添加一个新列到DataFrame。然后，由于LogisticRegression一个Estimator，Pipeline首先调用LogisticRegression.fit()拟合产生一个LogisticRegressionModel。如果管道有更多的Estimator，则在将DataFrame传递到下一个阶段之前，会先在DataFrame上调用LogisticRegressionModel的transform()方法。 PipeLine本身也是一个Estimator。因而，在工作流的fit()方法运行之后，它产生了一个PipelineModel，它也是一个Transformer。这个管道模型将在测试数据的时候使用。下图展示了这种用法。 在上图中，PipelineModel具有与原始Pipeline相同的阶段数，但是原始Pipeline中的所有估计器Estimators都变为变换器Transformers。当在测试数据集上调用PipelineModel的transform()方法时，数据按顺序通过拟合的管道。每个阶段的transform()方法更新数据集并将其传递到下一个阶段。Pipelines和PipelineModels有助于确保训练数据集和测试数据集通过相同的特征处理步骤。 理解Estimator，Transformer和Param相关API ：Estimator，Transformer，Params 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from pyspark.ml.linalg import Vectorsfrom pyspark.ml.classification import LogisticRegressionspark = SparkSession.builder.master(\"local\").appName(\"Estimator-Transformer-Param\").getOrCreate()# 准备训练数据集(label, features)元组training = spark.createDataFrame([ (1.0, Vectors.dense([0.0, 1.1, 0.1])), (0.0, Vectors.dense([2.0, 1.0, -1.0])), (0.0, Vectors.dense([2.0, 1.3, 1.0])), (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])# 创建一个LogisticRegression示例，也就是Estimatorlr = LogisticRegression(maxIter=10, regParam=0.01)# 打印出所以的参数和默认值信息print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")# 用训练数据集训练模型，这一步骤会使用到lr中的parametersmodel1 = lr.fit(training)# 现在，model1成为了一个Model（Estimator产生的transformer）# 我们查看一下拟合过程所用到的parametersprint(\"Model 1 was fit using parameters: \")print(model1.extractParamMap())# 修改参数paramMap = &#123;lr.maxIter: 20&#125;paramMap[lr.maxIter] = 30paramMap.update(&#123;lr.regParam: 0.1, lr.threshold: 0.55&#125;)# 合并参数paramMap2 = &#123;lr.probabilityCol: \"myProbability\"&#125; # 修改输出列名paramMapCombined = paramMap.copy()paramMapCombined.update(paramMap2)# 现在基于新的参数进行拟合model2 = lr.fit(training, paramMapCombined)print(\"Model 2 was fit using parameters: \")print(model2.extractParamMap())# 测试数据集test = spark.createDataFrame([ (1.0, Vectors.dense([-1.0, 1.5, 1.3])), (0.0, Vectors.dense([3.0, 2.0, -0.1])), (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])# 使用Transformer.transform()对测试数据进行预测# LogisticRegression.transform只会使用'features'列，myProbability列既是probability列，之前我们做过更改prediction = model2.transform(test)result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\").collect()for row in result: print(\"features=%s, label=%s -&gt; prob=%s, prediction=%s\" % (row.features, row.label, row.myProbability, row.prediction)) 可参考examples/src/main/python/ml/estimator_transformer_param_example.py Pipeline相关API ：Pipeline 1234567891011121314151617181920212223242526272829303132333435from pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import HashingTF, Tokenizer# 训练数据集(id, text, label)元组.training = spark.createDataFrame([ (0, \"a b c d e spark\", 1.0), (1, \"b d\", 0.0), (2, \"spark f g h\", 1.0), (3, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])# 配置pipeline，连接tokenizer，hashingTF和lrtokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")lr = LogisticRegression(maxIter=10, regParam=0.001)pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# 拟合model = pipeline.fit(training)# 测试数据集(id, text)元组test = spark.createDataFrame([ (4, \"spark i j k\"), (5, \"l m n\"), (6, \"spark hadoop spark\"), (7, \"apache hadoop\")], [\"id\", \"text\"])# 预测结果prediction = model.transform(test)selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")for row in selected.collect(): rid, text, prob, prediction = row print(\"(%d, %s) --&gt; prob=%s, prediction=%f\" % (rid, text, str(prob), prediction)) 可参考examples/src/main/python/ml/pipeline_example.py","categories":[],"tags":[{"name":"工作流","slug":"工作流","permalink":"https://oobspark.github.io/tags/工作流/"},{"name":"Pipelines","slug":"Pipelines","permalink":"https://oobspark.github.io/tags/Pipelines/"}]},{"title":"弹性分布式数据集-RDD","slug":"Resilient-Distributed-Datasets","date":"2018-04-09T07:55:30.000Z","updated":"2019-11-12T07:32:27.579Z","comments":true,"path":"2018/04/09/Resilient-Distributed-Datasets/","link":"","permalink":"https://oobspark.github.io/2018/04/09/Resilient-Distributed-Datasets/","excerpt":"","text":"弹性分布式数据集-RDD弹性分布式数据集（RDD）是一种具有容错特性的数据集合，能在Spark的各个组件间做出各类转换并无缝传递。 有两种方式创建RDD：并行化数据集合或是外部数据集合（文件，HDFS，HBase等）。 并行化集合（Parallelized Collections）12data = [1, 2, 3, 4, 5]distData = sc.parallelize(data) 外部数据集 Bash中创建文件 1echo -e \"zhangsan, 23\\nlisi, 25\\nwanger, 27\" &gt; /usr/local/spark/data.txt Pyspark Shell中创建基于文件的RDD 1distFile = sc.textFile(\"data.txt\") 当使用文件名时，所以的工作节点（Worker Nodes）都应该有能力访问到该文件。再次强调，本例基于Standalone。 RDD基本操作12345678lines = sc.textFile(\"data.txt\")lineLengths = lines.map(lambda s: len(s)) # 返回每行的长度作为一个集合lineLengths.collect() # [12, 8, 10]，返回元素集合lineLengths.first() # 12，返回第一个元素lineLengths.take(2) # [12, 8]，，返回第一个元素的集合lineLengths.count() # 3，集合元素总数lineLengths.reduce(lambda a, b: a + b) # 30，聚集函数，求和lineLengthsFiltered = lineLengths.filter(lambda x: x &gt;= 10) # [12, 10]，过滤出长度大于等于10的集合 更多可参考集合转换和操作 Spark中的函数传递Spark API中对函数传递有很大的依赖，主要有三种方式 Lambda表达式 123def doStuff(self, rdd): field = self.field return rdd.map(lambda s: field + s) 内部定义的函数 1234567if __name__ == \"__main__\": def myFunc(s): words = s.split(\" \") return len(words) sc = SparkContext(...) sc.textFile(\"file.txt\").map(myFunc) 模块中定义的函数","categories":[],"tags":[{"name":"弹性分布式数据集","slug":"弹性分布式数据集","permalink":"https://oobspark.github.io/tags/弹性分布式数据集/"},{"name":"RDD","slug":"RDD","permalink":"https://oobspark.github.io/tags/RDD/"}]},{"title":"Spark MLlib概述","slug":"machine-learning-library-guide","date":"2018-04-08T02:52:28.000Z","updated":"2019-11-12T07:32:27.584Z","comments":true,"path":"2018/04/08/machine-learning-library-guide/","link":"","permalink":"https://oobspark.github.io/2018/04/08/machine-learning-library-guide/","excerpt":"","text":"Spark MLlib概述Spark的MLlib，其目标是让机器学习实践更加简单且具可扩展性。提供的特性如下： 机器学习算法：分类，回归，聚类和协同过滤 特征提取：特征提取、转化，将维和选择 工作流（Pipelines）：构建，评估和调整机器学习工作流的工具 数据持久化：持久化/加载模型，算法和工作流 工具集：线性代数，统计学，数据操作的工具支持 MLlib的主要API值得注意的是，进入了Spark2.0版本之后，spark.mllib中基于RDD的API（ RDD-based APIs）也进入了维护模式，取而代之的是spark.ml中基于DataFrame的API（DataFrame-based API）。 依赖 线性代数库Breeze 基础线性代数子程序库Intel MKL或OpenBLAS NumPy &gt;=1.4","categories":[],"tags":[]},{"title":"搭建一个最简单的Spark集群","slug":"start-a-spark-cluster-in-standalone","date":"2018-04-07T09:12:16.000Z","updated":"2019-11-12T07:32:27.587Z","comments":true,"path":"2018/04/07/start-a-spark-cluster-in-standalone/","link":"","permalink":"https://oobspark.github.io/2018/04/07/start-a-spark-cluster-in-standalone/","excerpt":"","text":"Spark集群的工作原理 SparkContext可以连接到几种类型的集群管理器(Cluster Managers), 一旦连接，Spark就会获取集群中节点(Worker Node)上的执行者(Executor)，这些执行者是运行计算并为应用程序存储数据的进程。 然后，它将写好的应用程序代码（JAR包或Python文件）发送给执行者。 最后，SparkContext发送任务给执行者运行。 集群类型 StandaloneSpark框架本身也自带了完整的资源调度管理服务，可以独立部署到一个集群中，而不需要依赖其他系统来为其提供资源管理调度服务。 Apache MesosMesos是一种资源调度管理框架，可以为运行在它上面的Spark提供服务。Spark程序所需要的各种资源，都由Mesos负责调度。目前，Spark官方推荐采用这种模式，所以，许多公司在实际应用中也采用该模式。 Hadoop YARNSpark可运行于YARN之上，与Hadoop进行统一部署，资源管理和调度依赖YARN，分布式存储依赖HDFS。 KubernetesKubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。 同时也有第三方的集群模式，但尚未得到官方的支持，比如Nomad。 之后本教程将主要使用Standalone模式。 Standalone模式 启动集群的master服务 ./sbin/start-master.sh 启动集群的Workers ./sbin/start-slave.sh &lt;master-spark-URL&gt;，此处的master-spark-URL就是刚刚启动好的master服务地址，协议头为spark:://，你可以通过http://localhost:8080查看。本例中为spark://ubuntu:7077或spark://127.0.0.1:7077。 jps，可以查看集群信息（Master/Worker的进程ID） 停止集群的脚本也在./sbin下。","categories":[],"tags":[]},{"title":"Spark组件","slug":"spark-components","date":"2018-04-06T00:55:02.000Z","updated":"2019-11-12T07:32:27.586Z","comments":true,"path":"2018/04/06/spark-components/","link":"","permalink":"https://oobspark.github.io/2018/04/06/spark-components/","excerpt":"","text":"Spark组件 Spark Core：Spark Core包含Spark的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等。Spark建立在统一的抽象RDD之上，使其可以以基本一致的方式应对不同的大数据处理场景。 Spark SQL：Spark SQL允许开发人员直接处理RDD，同时也可查询Hive、HBase等外部数据源。Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行查询，并进行更复杂的数据分析。 Spark Streaming：Spark Streaming支持高吞吐量、可容错处理的实时流数据处理。 MLlib（机器学习）：MLlib提供了常用机器学习算法的实现，包括聚类、分类、回归、协同过滤等，降低了机器学习的门槛，开发人员只要具备一定的理论知识就能进行机器学习的工作； GraphX（图计算）：GraphX是Spark中用于图计算的API。 本教程将专注于MLlib组件的学习。","categories":[],"tags":[]},{"title":"Spark环境搭建","slug":"spark-environment","date":"2018-04-04T01:22:42.000Z","updated":"2019-11-12T07:32:27.586Z","comments":true,"path":"2018/04/04/spark-environment/","link":"","permalink":"https://oobspark.github.io/2018/04/04/spark-environment/","excerpt":"","text":"目标： 搭建Spark环境 写出第一个Spark程序并运行。 运行环境 ubuntu 16.04 python3 Java 1.8 Spark 2.1.0 Hadoop 2.7.5 ubuntu 16.04 基于vagrant vagrant box add ubuntu/xenial https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-vagrant.box vagrant init ubuntu/xenial 增加ip地址解析config.vm.network &quot;public_network&quot;, ip: &quot;192.168.100.110&quot; 增加内存上限config.vm.provider &quot;virtualbox&quot; do |vb| vb.memory = &quot;5248&quot; end vagrant up vagrant ssh python3 curl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;get-pip.py&quot; python3 get-pip.py sudo pip install ipython sudo apt install python3-pip sudo pip3 install ipython sudo pip3 install numpy Java 1.8 sudo apt-get update sudo apt-get install -y default-jre default-jdk Hadoop 2.7.5 wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.5/hadoop-2.7.5.tar.gz tar zxvf hadoop-2.7.5.tar.gz sudo mv hadoop-2.7.5 /usr/local/hadoop sudo chown vagrant /usr/local/hadoop Spark 2.3.0 wget https://www.apache.org/dyn/closer.lua/spark/spark-2.3.0/spark-2.3.0-bin-without-hadoop.tgz tar zxvf spark-2.3.0-bin-without-hadoop.tgz sudo mv spark-2.3.0-bin-without-hadoop /usr/local/spark sudo chown vagrant /usr/local/spark 在文件/usr/local/spark/conf/spark-env.sh文件开头增加:export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath) PATH vi ~/.bashrc，添加以下代码 12345678export PYSPARK_DRIVER_PYTHON=ipythonexport JAVA_HOME=/usr/lib/jvm/default-javaexport HADOOP_HOME=/usr/local/hadoopexport SPARK_HOME=/usr/local/sparkexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.6-src.zip:$PYTHONPATHexport PYSPARK_PYTHON=python3export PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$HADOOP_HOME/sbin:$PATH Spark 初体验 cd /usr/local/spark 执行./bin/pyspark，进入Spark shell。没有报错，表明安装正确。 运行Spark自带的例子。./bin/spark-submit examples/src/main/python/pi.py 10 第一个Spark程序 vi /usr/local/spark/code/wordCount.py 12345from pyspark import SparkContextsc = SparkContext( 'local', 'test')textFile = sc.textFile(\"file:///usr/local/spark/README.md\")wordCount = textFile.flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a, b : a + b)wordCount.foreach(print) 至此，本文也到了该完结的时候，在敲下这个命令之后：python3 code/wordCount.py","categories":[],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://oobspark.github.io/tags/Spark/"},{"name":"环境搭建","slug":"环境搭建","permalink":"https://oobspark.github.io/tags/环境搭建/"}]},{"title":"Spark由浅入深","slug":"intro","date":"2018-04-02T03:21:43.000Z","updated":"2019-11-12T07:32:27.583Z","comments":true,"path":"2018/04/02/intro/","link":"","permalink":"https://oobspark.github.io/2018/04/02/intro/","excerpt":"","text":"本系列教程由浅入深介绍Spark在大数据中的实践和应用，让大家对大数据时代能有更清晰，更真切的认识。","categories":[],"tags":[{"name":"教程简介","slug":"教程简介","permalink":"https://oobspark.github.io/tags/教程简介/"},{"name":"Spark","slug":"Spark","permalink":"https://oobspark.github.io/tags/Spark/"}]}]}